{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo 4: Vendas - Detec\u00e7\u00e3o de Fraude em Tempo Real (Spark ML)\n",
    "\n",
    "Este notebook demonstra o uso de **Machine Learning** com **Spark Streaming** para identificar fraudes em transa\u00e7\u00f5es de vendas.\n",
    "\n",
    "**Cen\u00e1rio**: Treinar um modelo K-Means para agrupar transa\u00e7\u00f5es normais e detectar anomalias (fraudes) em tempo real com base no valor da transa\u00e7\u00e3o e dist\u00e2ncia geogr\u00e1fica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configura\u00e7\u00e3o do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar Java\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "# Baixar e Instalar Spark\n",
    "!wget https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz && tar xf spark-3.5.0-bin-hadoop3.tgz\n",
    "\n",
    "# Baixar e Instalar Kafka\n",
    "!wget https://archive.apache.org/dist/kafka/3.6.1/kafka_2.13-3.6.1.tgz && tar xf kafka_2.13-3.6.1.tgz\n",
    "\n",
    "# Instalar pacotes Python\n",
    "!pip install -q findspark pyspark kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Iniciar Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd kafka_2.13-3.6.1\n",
    "bin/zookeeper-server-start.sh -daemon config/zookeeper.properties\n",
    "sleep 5\n",
    "bin/kafka-server-start.sh -daemon config/server.properties\n",
    "sleep 5\n",
    "bin/kafka-topics.sh --create --topic transactions --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Treinamento do Modelo (K-Means)\n",
    "Simulamos um hist\u00f3rico de dados 'normais' para treinar o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import random\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FraudDetection\").getOrCreate()\n",
    "\n",
    "# Gerar dados hist\u00f3ricos (Normais: valor baixo/m\u00e9dio, dist\u00e2ncia baixa)\n",
    "# Valores: 10-500, Dist\u00e2ncia: 0-50\n",
    "data = []\n",
    "for _ in range(1000):\n",
    "    data.append((float(random.randint(10, 500)), float(random.randint(0, 50))))\n",
    "\n",
    "# Adicionar algumas anomalias para o 'treino' n\u00e3o ficar viciado demais, ou deixar puramente normal\n",
    "# Na verdade, K-Means vai achar o centroide dos dados normais.\n",
    "\n",
    "df_train = spark.createDataFrame(data, [\"amount\", \"distance\"])\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"amount\", \"distance\"], outputCol=\"features\")\n",
    "df_train_vec = assembler.transform(df_train)\n",
    "\n",
    "# Treinar KMeans com k=2 (Pequenas/Normais vs Grandes/Distantes se houver, ou for\u00e7ar apenas 1 cluster normal)\n",
    "# Vamos supor k=1 para simplificar 'o que \u00e9 normal' ou k=5 clusters de comportamento padr\u00e3o.\n",
    "kmeans = KMeans(k=2, seed=1)\n",
    "model = kmeans.fit(df_train_vec)\n",
    "centers = model.clusterCenters()\n",
    "print(\"Centros dos Clusters:\", centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Produtor de Transa\u00e7\u00f5es em Tempo Real\n",
    "Gera transa\u00e7\u00f5es misturando leg\u00edtimas e fraudes (valor alt\u00edssimo ou muito longe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from kafka import KafkaProducer\n",
    "import threading\n",
    "\n",
    "def generate_transactions():\n",
    "    producer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n",
    "                             value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n",
    "    try:\n",
    "        for _ in range(100):\n",
    "            if random.random() > 0.9: # 10% de chance de fraude\n",
    "                txn = {\"amount\": float(random.randint(1000, 5000)), \"distance\": float(random.randint(100, 500))}\n",
    "            else:\n",
    "                txn = {\"amount\": float(random.randint(10, 500)), \"distance\": float(random.randint(0, 50))}\n",
    "            \n",
    "            producer.send('transactions', value=txn)\n",
    "            time.sleep(0.5)\n",
    "    finally:\n",
    "        producer.close()\n",
    "\n",
    "thread = threading.Thread(target=generate_transactions)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detec\u00e7\u00e3o em Tempo Real\n",
    "Consome do Kafka e aplica o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile kafka_consumer.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import random\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FraudDetection\").getOrCreate()\n",
    "\n",
    "# Gerar dados hist\u00f3ricos (Normais: valor baixo/m\u00e9dio, dist\u00e2ncia baixa)\n",
    "# Valores: 10-500, Dist\u00e2ncia: 0-50\n",
    "data = []\n",
    "for _ in range(1000):\n",
    "    data.append((float(random.randint(10, 500)), float(random.randint(0, 50))))\n",
    "\n",
    "# Adicionar algumas anomalias para o 'treino' n\u00e3o ficar viciado demais, ou deixar puramente normal\n",
    "# Na verdade, K-Means vai achar o centroide dos dados normais.\n",
    "\n",
    "df_train = spark.createDataFrame(data, [\"amount\", \"distance\"])\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"amount\", \"distance\"], outputCol=\"features\")\n",
    "df_train_vec = assembler.transform(df_train)\n",
    "\n",
    "# Treinar KMeans com k=2 (Pequenas/Normais vs Grandes/Distantes se houver, ou for\u00e7ar apenas 1 cluster normal)\n",
    "# Vamos supor k=1 para simplificar 'o que \u00e9 normal' ou k=5 clusters de comportamento padr\u00e3o.\n",
    "kmeans = KMeans(k=2, seed=1)\n",
    "model = kmeans.fit(df_train_vec)\n",
    "centers = model.clusterCenters()\n",
    "print(\"Centros dos Clusters:\", centers)",
    "\n# --- Streaming Logic ---\n",
    "from pyspark.sql.functions import from_json, col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"amount\", DoubleType()),\n",
    "    StructField(\"distance\", DoubleType())\n",
    "])\n",
    "\n",
    "df_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"transactions\") \\\n",
    "    .load()\n",
    "\n",
    "txns = df_stream.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Obs: Streaming MLPipelins s\u00e3o complexos. Uma abordagem simples \u00e9 usar 'foreachBatch' ou aplicar o modelo transformado se suportado.\n",
    "# O K-MeansModel do Spark suporta transform em streaming?\n",
    "# Na vers\u00e3o 3.5, sim, 'transform' pode ser usado.\n",
    "\n",
    "# Precisa remontar o VectorAssembler nas features de streaming\n",
    "# Nota: Para usar ML Transformers em streaming, \u00e9 preciso que eles sejam stateless ou o modelo j\u00e1 esteja fitado.\n",
    "\n",
    "assembler_stream = VectorAssembler(inputCols=[\"amount\", \"distance\"], outputCol=\"features\")\n",
    "txns_vec = assembler_stream.transform(txns)\n",
    "\n",
    "predictions = model.transform(txns_vec)\n",
    "\n",
    "# Vamos considerar 'fraude' se a predi\u00e7\u00e3o cair em um cluster espec\u00edfico OU se tivermos l\u00f3gica de dist\u00e2ncia.\n",
    "# Para este exemplo simples, vamos apenas exibir o cluster predito.\n",
    "# Se tiv\u00e9ssemos definido que cluster 1 s\u00e3o 'valores altos', filtrar\u00edamos prediction == 1.\n",
    "\n",
    "query = predictions.select(\"amount\", \"distance\", \"prediction\") \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "\nquery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1 kafka_consumer.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}