{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 29: Healthcare V2\n",
    "\n",
    "Patient risk analysis.\n",
    "\n",
    "**Architecture**:\n",
    "*   **Source**: Kafka (Streaming)\n",
    "*   **Enrichment/State**: Redis (Fast lookups)\n",
    "*   **Sink**: MongoDB (Document storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\nInstall Java, Spark, Kafka, Redis, MongoDB, and Python drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Java\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "# Install Spark & Kafka\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
    "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
    "!wget -q https://archive.apache.org/dist/kafka/3.6.1/kafka_2.13-3.6.1.tgz\n",
    "!tar xf kafka_2.13-3.6.1.tgz\n",
    "\n",
    "# Install Redis & MongoDB\n",
    "!apt-get install redis-server -qq > /dev/null\n",
    "!wget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc | apt-key add -\n",
    "!echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/6.0 multiverse\" | tee /etc/apt/sources.list.d/mongodb-org-6.0.list\n",
    "!apt-get update -qq > /dev/null\n",
    "!apt-get install -y mongodb-org -qq > /dev/null\n",
    "\n",
    "# Install Python Libs\n",
    "!pip install -q findspark pyspark kafka-python redis pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Start Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Start Redis\n",
    "service redis-server start\n",
    "\n",
    "# Start MongoDB (Background)\n",
    "mkdir -p /data/db\n",
    "mongod --fork --logpath /var/log/mongodb.log --bind_ip 127.0.0.1\n",
    "\n",
    "# Start Kafka\n",
    "cd kafka_2.13-3.6.1\n",
    "bin/zookeeper-server-start.sh -daemon config/zookeeper.properties\n",
    "sleep 5\n",
    "bin/kafka-server-start.sh -daemon config/server.properties\n",
    "sleep 5\n",
    "bin/kafka-topics.sh --create --topic input-topic --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Connections\n",
    "import redis\n",
    "from pymongo import MongoClient\n",
    "\n",
    "try:\n",
    "    r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "    r.set('test', 'connected')\n",
    "    print(f\"Redis: {r.get('test').decode('utf-8')}\")\n",
    "except Exception as e: print(f\"Redis Error: {e}\")\n",
    "\n",
    "try:\n",
    "    m = MongoClient('localhost', 27017)\n",
    "    print(f\"MongoDB: {m.list_database_names()}\")\n",
    "except Exception as e: print(f\"Mongo Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Producer (Simulated Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json, time, random\n",
    "producer = KafkaProducer(bootstrap_servers=['localhost:9092'], value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n",
    "for _ in range(500):\n",
    "    data = {'player': f'p{random.randint(1,100)}', 'points': random.randint(10, 100)}\n",
    "    producer.send('input-topic', value=data)\n",
    "    time.sleep(0.05)\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Consumer (Spark + Redis + Mongo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile kafka_consumer.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, udf, struct, to_json\n",
    "from pyspark.sql.types import *\n",
    "import redis\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "# --- Init Spark ---\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AdvancedIntegration\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.output\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# --- Redis & Mongo Connection Helpers (Executed on Workers) ---\n",
    "def get_redis():\n",
    "    return redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "def write_to_mongo_redis(batch_df, batch_id):\n",
    "    # This function runs on the driver, but operations should be optimized for workers if large scale.\n",
    "    # For simplicity in this example, we collect minor batches or use foreachPartition inside.\n",
    "    \n",
    "    data = batch_df.collect()\n",
    "    if not data: return\n",
    "    \n",
    "    r_client = get_redis()\n",
    "    m_client = MongoClient('localhost', 27017)\n",
    "    db = m_client['streaming_db']\n",
    "    collection = db['output_collection']\n",
    "    \n",
    "    print(f\"Processing Batch {batch_id} with {len(data)} records\")\n",
    "    \n",
    "    for row in data:\n",
    "        record = row.asDict()\n",
    "        \n",
    "\n                val = json.loads(record['value'])\n                pid = val['patient_id']\n                bpm = val['bpm']\n                \n                # Redis: Get max limit for patient (default 120)\n                limit_key = f\"limit:{pid}\"\n                max_bpm = r_client.get(limit_key)\n                if not max_bpm: max_bpm = 120\n                else: max_bpm = int(max_bpm)\n                \n                risk = \"LOW\"\n                if bpm > max_bpm: risk = \"CRITICAL\"\n                elif bpm > 100: risk = \"ELEVATED\"\n                \n                record['risk_level'] = risk\n                record['parsed_data'] = val\n        \n",
    "        # Save to Mongo\n",
    "        try:\n",
    "            collection.insert_one(record)\n",
    "        except Exception as e: print(e)\n",
    "            \n",
    "    m_client.close()\n",
    "    r_client.close()\n",
    "\n# --- Stream Definition ---\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"value\", StringType()),\n",
    "    StructField(\"meta\", StringType())\n",
    "     # Simplified schema for template, can be overridden\n",
    "])\n",
    "\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"input-topic\") \\\n",
    "    .load()\n",
    "\n",
    "# Generic parsing - tailored in specific notebooks if needed, but here we assume JSON value\n",
    "parsed = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Apply Custom Processing\n",
    "query = parsed.writeStream \\\n",
    "    .foreachBatch(write_to_mongo_redis) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.mongodb.spark:mongo-spark-connector_2.12:10.2.1 kafka_consumer.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}