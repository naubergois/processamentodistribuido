{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 28: Social Trends\n",
    "\n",
    "Hashtag tracking.\n",
    "\n",
    "**Architecture**:\n",
    "*   **Source**: Kafka (Streaming)\n",
    "*   **Enrichment/State**: Redis (Fast lookups)\n",
    "*   **Sink**: MongoDB (Document storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\nInstall Java, Spark, Kafka, Redis, MongoDB, and Python drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Java\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "# Install Spark & Kafka\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
    "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
    "!wget -q https://archive.apache.org/dist/kafka/3.6.1/kafka_2.13-3.6.1.tgz\n",
    "!tar xf kafka_2.13-3.6.1.tgz\n",
    "\n",
    "# Install Redis & MongoDB\n",
    "!apt-get install redis-server -qq > /dev/null\n",
    "!wget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc | apt-key add -\n",
    "!echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/6.0 multiverse\" | tee /etc/apt/sources.list.d/mongodb-org-6.0.list\n",
    "!apt-get update -qq > /dev/null\n",
    "!apt-get install -y mongodb-org -qq > /dev/null\n",
    "\n",
    "# Install Python Libs\n",
    "!pip install -q findspark pyspark kafka-python redis pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Start Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Start Redis\n",
    "service redis-server start\n",
    "\n",
    "# Start MongoDB (Background)\n",
    "mkdir -p /data/db\n",
    "mongod --fork --logpath /var/log/mongodb.log --bind_ip 127.0.0.1\n",
    "\n",
    "# Start Kafka\n",
    "cd kafka_2.13-3.6.1\n",
    "bin/zookeeper-server-start.sh -daemon config/zookeeper.properties\n",
    "sleep 5\n",
    "bin/kafka-server-start.sh -daemon config/server.properties\n",
    "sleep 5\n",
    "bin/kafka-topics.sh --create --topic input-topic --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Connections\n",
    "import redis\n",
    "from pymongo import MongoClient\n",
    "\n",
    "try:\n",
    "    r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "    r.set('test', 'connected')\n",
    "    print(f\"Redis: {r.get('test').decode('utf-8')}\")\n",
    "except Exception as e: print(f\"Redis Error: {e}\")\n",
    "\n",
    "try:\n",
    "    m = MongoClient('localhost', 27017)\n",
    "    print(f\"MongoDB: {m.list_database_names()}\")\n",
    "except Exception as e: print(f\"Mongo Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Producer (Simulated Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json, time, random\n",
    "producer = KafkaProducer(bootstrap_servers=['localhost:9092'], value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n",
    "tags = ['#tech', '#news', '#ai', '#crypto']\n",
    "for _ in range(500):\n",
    "    data = {'post_id': random.randint(1000,9999), 'tag': random.choice(tags)}\n",
    "    producer.send('input-topic', value=data)\n",
    "    time.sleep(0.05)\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Consumer (Spark + Redis + Mongo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile kafka_consumer.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, udf, struct, to_json\n",
    "from pyspark.sql.types import *\n",
    "import redis\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "# --- Init Spark ---\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AdvancedIntegration\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# --- Redis & Mongo Connection Helpers (Executed on Workers) ---\n",
    "def get_redis():\n",
    "    return redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "def write_to_mongo_redis(batch_df, batch_id):\n",
    "    # This function runs on the driver, but operations should be optimized for workers if large scale.\n",
    "    # For simplicity in this example, we collect minor batches or use foreachPartition inside.\n",
    "    \n",
    "    data = batch_df.collect()\n",
    "    if not data: return\n",
    "    \n",
    "    r_client = get_redis()\n",
    "    m_client = MongoClient('localhost', 27017)\n",
    "    db = m_client['streaming_db']\n",
    "    collection = db['output_collection']\n",
    "    \n",
    "    print(f\"Processing Batch {batch_id} with {len(data)} records\")\n",
    "    \n",
    "    for row in data:\n",
    "        record = row.asDict()\n",
    "        \n",
    "\n        val = json.loads(record['value'])\n        tag = val['tag']\n        \n        # Redis: Atomic Increment\n        total = r_client.hincrby(\"trends\", tag, 1)\n        \n        record['tag_total'] = total\n        record['parsed_data'] = val\n\n",
    "        # Save to Mongo\n",
    "        try:\n",
    "            collection.insert_one(record)\n",
    "        except Exception as e: print(e)\n",
    "            \n",
    "    m_client.close()\n",
    "    r_client.close()\n",
    "\n# --- Stream Definition ---\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"value\", StringType()),\n",
    "    StructField(\"meta\", StringType())\n",
    "     # Simplified schema for template, can be overridden\n",
    "])\n",
    "\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"input-topic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Generic parsing - tailored in specific notebooks if needed, but here we assume JSON value\n",
    "parsed = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Apply Custom Processing\n",
    "query = parsed.writeStream \\\n",
    "    .foreachBatch(write_to_mongo_redis) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.mongodb.spark:mongo-spark-connector_2.12:10.2.1 kafka_consumer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Output (MongoDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pymongo import MongoClient\n",
    "\n",
    "print(\"Waiting 30s for data pipeline to flush...\")\n",
    "time.sleep(30)\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['streaming_db']\n",
    "collection = db['output_collection']\n",
    "\n",
    "count = collection.count_documents({})\n",
    "print(f\"Total Documents in MongoDB: {count}\")\n",
    "\n",
    "if count > 0:\n",
    "    print(\"Sample Data:\")\n",
    "    for doc in collection.find().limit(5):\n",
    "        print(doc)\n",
    "else:\n",
    "    print(\"No data found yet. Ensure the Spark job is running and producing data.\")\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}